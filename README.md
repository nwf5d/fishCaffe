# <center>我该怎么学深度学习</center>

​	如果我们要使用一个东西而不去了解其内在的原理是不高明的。当然这句话不是拿来抬杠用的，对于外行人来说完全没有必须去了解其中的技术细节，但是对于从事这一行的人来说了解技术的内在原理和细节还是非常重要的。就像侯捷老师说的，源码之前，了无秘密。认真读过分析过源码就会对这一项技术会有更加深刻的理解。最近得空完整的阅读了一遍caffe的源码，对于我来说看caffe源码的过程也是一种自我学习提高的过程，当然自己也想把整个学习过程给记录下来供以后复习使用，caffe整个框架像一张大网一般，涵盖了很多的知识点，学习这么一个系统级别的源代码一定要保持耐心，切勿急功近利。在学习的时候可以多多参考网络上的资源，深度学习的火爆的好处就是开源社区非常活跃，你的问题一般都能在网络上面找到答案。

​	当然学习caffe的时候一定要用非常好的C++基础，不然源代码会看的非常的吃力。说到这里就不禁的想推荐几门侯捷老师开设的课程，混C++的应该听说过侯捷老师的大名，如果是刚入坑C++的可以给大家推荐以下系列的C++课程:

[《面向对象高级编程》](https://www.bilibili.com/video/av27135524?from=search&seid=14996345474654432100)(侯捷)，这门课主要讲解的面向对象的基本概念和编写规范，课程中以complex类和string类这两大类介绍了面向对象编程的基本规范，当然在此基础上非常清晰的介绍了虚函数、多态、继承、委托、组合等非常重要的C++面向对象的概念。如果之前C语言还过关的话，学习这门课程可以快速的了解C++面向对象的核心概念。

[《STL 体系结构与内核分析》](https://www.bilibili.com/video/av45108908?from=search&seid=14996345474654432100)(侯捷)，学习C++必定绕不过STL的学习，这是C++的一神兵利器。在这一门课中，侯捷老师从源码的角度来给大家分析STL中的分配器、迭代器、容器、算法、适配器等核心部件的构成原理和实现方法。侯捷老师对STL的学习分为三种境界，第一种境界是熟练使用STL，第二种境界是了解泛型技术内涵和掌握STL的原理及实现，第三种境界是扩充STL。达到第三种境界的人写起STL代码来虎虎生风，是第一种境界的人所不能望其项背的。个人感觉看完该视频，同时认真的阅读过侯捷老师写的《STL源码剖析》一书应该可以达到第二种境界的水平。

[《设计模式》](https://www.bilibili.com/video/av24176315?from=search&seid=7706330993384532458)(李建忠)，设计模式又是更上一种层次了。设计模式在软件工程种有非常重要的地位，一旦设计不合理可能会导致无穷的后患。当然在caffe种我们只是简单的使用了工厂设计模式，但是借此机会看该视频学习学习设计模式还是非常的有必要的。

​	嗯，学完上述的C++课程应该可以能够较为顺利的读一半caffe源码了。为什么说是一半caffe源码，因为目前很多深度学习框架都是部署在带有GPU的机器上的，caffe的模型训练加速主要是通过GPU来完成的。如果想了解整个加速过程，那就得掌握GPU编程即cuda编程，对于cuda编程的学习可以推荐下面的学习资源。

[《CUDA C Programming Guide》](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)和《[CUDA C Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#abstract)》，这两个是英伟达的官方学习资料，学完这两个文档应该可以看得懂的caffe中的cuda代码。如果感觉对看文档比较吃力，可以推荐看台湾新竹清华大学开设的[《平行程式课程》](https://www.bilibili.com/video/av40579783?from=search&seid=10877917092918778533)，里面花了很多的课时来讲CUDA程序编写和优化。

​	我想学习完上述的知识之后看caffe源码应该就没啥太大问题了，比较硬核的编程知识都可以从上面的课程中学习到，剩下的一些部分就是一些第三方库的使用，这个学习起来难度都比较低，看看官方的文档同时自己跑几个小demo就应该知道如何使用了。剩下的硬核知识就是和深度学习相关的知识了，在深度学习如此火爆的时代，深度学习的资料真是多的数不过来。所以这就推荐一个资源，那就是吴恩达老师在coursera上开设的深度学习课程，课程中对深度学习的数学原理有一个非常清晰的介绍，同时其课程作业还要求我们去实现全连接神经网络（DNN）、卷积神经网络（CNN）以及循环神经网络的（RNN）的前向和反向传播过程。这一部分的知识对于学习caffe框架来说是非常重要的，可以说是整个caffe框架中最难搞的部分。

​	ok，我想有了上述的知识储备，那么看caffe框架就简单很多了。我觉的看框架的时候要不停的去修改调试，对于一些比较难理解的部分尝试自己用画图的形式一步一步将其展现出来，当然caffe框架是比较早期的框架，可能对于目前的一些新模型的支持度可能不够，在看源码框架的时候也可以自己动手去做一些添加层、求解器等动作。只有不停的动手，才能做到真正的理解，废话不多说，后面就开始记录我的学习时刻。

## <center>第一部分：CUDA编程及优化简介</center>



## <center>第二部分：caffe使用的第三方库简介</center>

## <center>第三部分：caffe中的一些基础设施</center>

## <center>第四部分：caffe的主存模型和IO模型</center>

### 统一内存管理(SyncedMemory类)

### 数据管理（Blob类）

### 阻塞队列 （BlockingQueue类）

## <center>第四部分：网络生成的全过程</center>



## <center>第五部分：不同算法模块的实现</center>

### 数据初始化（Filler类）

### 激活函数层

### 求解器（SGDSolver类）

### im2col函数

### 全连接层（InnerProductLayer）

### 池化层（PoolingLayer）

### 卷积层（常规卷积、空洞卷积、反卷积）

### 损失层

### RNN层

## <center>第六部分：对照源码理解一些基础paper</center>

### AlexNet

### VGGNet

### Batch Normalization

### ResNet

### GoogLeNet

### FCN

## <center>第七部分：添加目标检测模块</center>

## <center>第八部分：尝试玩一下核弹厂的核弹吧</center>

