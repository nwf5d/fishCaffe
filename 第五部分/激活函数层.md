### 激活函数层

#### NeuronLayer类及其继承关系

本节主要介绍NeuronLayer类及其派生的子类关系。在caffe的include/caffe/layers/neuron_layer.hpp中写到，neuron_layer类及其派生子类的输入blob和输出blob的size都是一样的，同时输出blob的每个元素只和输入blob所对应的元素相关。

![NeuronLayer的继承关系](C:\Users\yupei\Desktop\caffe源码\图片\NeuronLayer的继承关系.png)

在caffe中目前派生的NeuronLayer子类有：AbsValLayer，BNLLLayer，ClipLayer，DropoutLayer，ELULayer，ExpLayer，LogLayer，PReLULayer，PowerLayer，ReLULayer，SigmoidLayer，SwishLayer，TanHLayer和ThresholdLayer，下面将会对各个派生类进行相应的介绍和测试。

#### NeuronLayer类

(include/caffe/layers/neuron_layer.hpp)

```c++
template <typename Dtype>
class NeuronLayer : public Layer<Dtype> {
public:
    explicit NeuronLayer(const LayerParameter& param)
	: Layer<Dtype>(param) {}
    virtual void Reshape(const vector<Blob<Dtype>*>& bottom,
	    const vector<Blob<Dtype>*>& top);
    virtual inline int ExactNumBottomBlobs() const { return 1; }    //bottom只有一个blob
    virtual inline int ExactNumTopBlobs() const { return 1; }      //top也只有一个blob
};
```

NeuronLayer重载了基类Layer的Reshape函数、ExactNumBottomBlobs函数和ExactNumTopBlobs函数。我们可以看到在NeuronLayer中bottom和top数组维度都是1，说明NeuronLayer及其派生类中是不涉及任何学习参数的，只是简单的数学映射函数的，所以我们暂称这种Layer叫做函数Layer。同时它要求经过NeruonLayer类的bottom blob和top blob的维度要一样，我们可以从其.cpp文件中可以看出来。

```c++
template <typename Dtype>
void NeuronLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
        const vector<Blob<Dtype>*>& top) {
    top[0]->ReshapeLike(*bottom[0]);         //bottom blob和input blob的维度要一样
}
```

#### AbsValLayer类

该类比较简单，就不做具体分析。

其前向传播函数为：
$$
y = |x|
$$
反向传播函数为：
$$
     y'=\begin{equation}  
\left\{  
             \begin{array}{**lr**}  
        1, & 0\leq x\\  
  -1, &x \lt 0
             \end{array}  
\right.  
\end{equation}
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
  \frac{\partial E}{\partial x}=\begin{equation}  
\left\{  
             \begin{array}{**lr**}  
           \frac{\partial E}{\partial y}, & 0\leq x\\  
             -\frac{\partial E}{\partial y}, &x \lt 0
             \end{array}  
\right.  
\end{equation}
$$
具体的测试代码可以见(fish_test/test_absval_layer.hpp)

#### BNLLLayer类

前向传播函数：
$$
y =\begin{equation}  
\left\{  
             \begin{array}{**lr**}  
 x + \log(1 + \exp(-x)), &x\gt0\\
\log(1 + \exp(x)),&x\leq0
             \end{array}  
\right.  
\end{equation}
$$
反向传播函数为：
$$
y' = \begin{equation}  
\left\{  
             \begin{array}{**lr**}  
exp(50)/(1 + exp(50)), &x\gt50.0\\
exp(x)/(1 + exp(x)),&x\leq0
             \end{array}  
\right.  
\end{equation}
$$
我目前不是很清楚这个bnllayer具体是干嘛的，对于其求导也表示非常好奇。

反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x}=\begin{equation}  
\left\{  
             \begin{array}{**lr**}  
\frac{\partial E}{\partial y} * exp(50)/(1 + exp(50)), &x\gt50.0\\
\frac{\partial E}{\partial y} * exp(x)/(1 + exp(x)),&x\leq0
             \end{array}  
\right.  
\end{equation}
$$

#### ClipLayer类

前向传播函数为：
$$
y=\max(min, \min(max, x))
$$
反向传播函数为：
$$
y'= \left\{
          \begin{array}{lr}
       0 & \mathrm{if} \; x < min \vee x > max \\
      1& \mathrm{if} \; x \ge min \wedge x \le max
  \end{array} \right.
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x} = \left\{
          \begin{array}{lr}
       0 & \mathrm{if} \; x < min \vee x > max \\
      \frac{\partial E}{\partial y} & \mathrm{if} \; x \ge min \wedge x \le max
  \end{array} \right.
$$

#### DropoutLayer类

dropout是为了防止过拟合的时候使用的，但是它一般在全连接层的时候使用，而且我们只在训练的时候开启dropout，在测试的时候会将整个dropout层给关闭掉。

前向传播过程
$$
y_{\mbox{train}} = \left\{
         \begin{array}{ll}
       \frac{x}{1 - p} & \mbox{if } u > p, & u \sim U(0, 1) \\
     0 & \mbox{otherwise}
    \end{array} \right.
$$
反向传播过程
$$
y_{\mbox{train}}’ = \left\{
         \begin{array}{ll}
       \frac{1}{1 - p} & \mbox{if } u > p, & u \sim U(0, 1) \\
     0 & \mbox{otherwise}
    \end{array} \right.
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y_{\mbox{train}}}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x}= \left\{
         \begin{array}{ll}
      \frac{\partial E}{\partial y_{\mbox{train}}}* \frac{1}{1 - p} & \mbox{if } u > p, & u \sim U(0, 1) \\
     0 & \mbox{otherwise}
    \end{array} \right.
$$
注意在dropout的实现中我们需要通过伯努利分布来生成0-1数。

#### ELULayer类

前向传播过程
$$
y = \left\{
   \begin{array}{lr}
            x                  & \mathrm{if} \; x > 0 \\
            \alpha (\exp(x)-1) & \mathrm{if} \; x \le 0
        \end{array} \right.
$$
反向传播过程
$$
y'= \left\{
         \begin{array}{lr}
                 1           & \mathrm{if} \; x > 0 \\
                 y + \alpha  & \mathrm{if} \; x \le 0
             \end{array} \right.
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x}= \left\{
         \begin{array}{lr}
                 \frac{\partial E}{\partial y}           & \mathrm{if} \; x > 0 \\
                 \frac{\partial E}{\partial y}*(y + \alpha)  & \mathrm{if} \; x \le 0
             \end{array} \right.
$$

#### ExpLayer类

前向传播过程
$$
y = \gamma ^ {\alpha x + \beta}
$$
反向传播过程
$$
y' =
y \alpha \log_e(\gamma)
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x}=\frac{\partial E}{\partial y}*
y \alpha \log_e(\gamma)
$$

#### LogLayer类

前向传播过程
$$
y = log_{\gamma}(\alpha x + \beta)
$$
反向传播过程
$$
y' = y \alpha \log_e(gamma)
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x} =
     \frac{\partial E}{\partial y} y \alpha \log_e(gamma)
$$

#### PReLULayer类

前向传播过程
$$
y_i = \max(0, x_i) + a_i \min(0, x_i)
$$
反向传播过程：
$$
y'= \left\{
  \begin{array}{lr}
          a_i & \mathrm{if} \; x_i \le 0 \\
             1& \mathrm{if} \; x_i > 0
           \end{array} \right.
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x_i} = \left\{
  \begin{array}{lr}
          a_i \frac{\partial E}{\partial y_i} & \mathrm{if} \; x_i \le 0 \\
               \frac{\partial E}{\partial y_i} & \mathrm{if} \; x_i > 0
           \end{array} \right.
$$

#### PowerLayer类

前向传播过程：
$$
y = (\alpha x + \beta) ^ \gamma
$$
反向传播过程：
$$
y'=

    \alpha \gamma (\alpha x + \beta) ^ {\gamma - 1} =

          \frac{\alpha \gamma y}{\alpha x + \beta}
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x} =
    \frac{\partial E}{\partial y}
    \alpha \gamma (\alpha x + \beta) ^ {\gamma - 1} =
      \frac{\partial E}{\partial y}
          \frac{\alpha \gamma y}{\alpha x + \beta}
$$

#### ReLULayer类

前向传播：
$$
y = \max(0, x)
$$
反向传播：
$$
y' = \left\{
 \begin{array}{lr}
       0 & \mathrm{if} \; x \le 0 \\
           1& \mathrm{if} \; x > 0
    \end{array} \right.
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x} = \left\{
 \begin{array}{lr}
       0 & \mathrm{if} \; x \le 0 \\
            \frac{\partial E}{\partial y} & \mathrm{if} \; x > 0
    \end{array} \right.
$$

#### SigmoidLayer类

前向传播：
$$
y = (1 + \exp(-x))^{-1}
$$
反向传播：
$$
y'
         = y (1 - y)
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x}
         = \frac{\partial E}{\partial y} y (1 - y)
$$

#### SwishLayer类

前向传播：
$$
y = x \sigma (\beta x)
$$
反向传播：
$$
y'
         = (\beta y +
       \sigma (\beta x)(1 - \beta y))
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x}
        = \frac{\partial E}{\partial y}(\beta y +
   \sigma (\beta x)(1 - \beta y))
$$

#### TanHLayer类

前向传播：
$$
y = \frac{\exp(2x) - 1}{\exp(2x) + 1}
$$
反向传播：
$$
y'
      =
         \left(1 - \left[\frac{\exp(2x) - 1}{exp(2x) + 1} \right]^2 \right)
        =(1 - y^2)
$$
反向传播传到该层的的时候，会在top blob的diff中存有梯度为：
$$
\frac{\partial E}{\partial y}
$$
那传到该层的bottom blob的时候中的diff的梯度为：
$$
\frac{\partial E}{\partial x}
      = \frac{\partial E}{\partial y}
          \left(1 - \left[\frac{\exp(2x) - 1}{exp(2x) + 1} \right]^2 \right)
          = \frac{\partial E}{\partial y} (1 - y^2)
$$
ThresholdLayer类

前向传播
$$
y = \left\{
   \begin{array}{lr}
 0 & \mathrm{if} \; x \le t \\
         1 & \mathrm{if} \; x > t
   \end{array} \right.
$$
注意：并没有反向传播。



